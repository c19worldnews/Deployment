{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deployment_V7_github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neerja198/Deployment/blob/main/deployment_V7_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken '21yoqTo1vqdetWnr3rIu8NMUtpY_4NyUeZLB6NjxJpPNCzzC2'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi6_eG4qSAIr",
        "outputId": "96ab924c-4230-42df-8f1e-f4b6ee0900de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKUJSzcM1Vfl",
        "outputId": "682a06eb-e4d9-4a67-bd47-9c64c93c1818"
      },
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "#!pip install selenium\n",
        "# set options to be headless, ..\n",
        "\n",
        "#print(wd.page_source) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,461 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,446 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [691 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,819 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,898 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [933 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,228 kB]\n",
            "Get:23 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.9 kB]\n",
            "Fetched 12.8 MB in 4s (3,590 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "58 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 94.0 MB of archives.\n",
            "After this operation, 324 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 95.0.4638.69-0ubuntu0.18.04.1 [1,135 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 95.0.4638.69-0ubuntu0.18.04.1 [83.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 95.0.4638.69-0ubuntu0.18.04.1 [4,249 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 95.0.4638.69-0ubuntu0.18.04.1 [4,986 kB]\n",
            "Fetched 94.0 MB in 4s (23.3 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_95.0.4638.69-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
            "\u001b[K     |████████████████████████████████| 958 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure]~=1.26\n",
            "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 23.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.25.11\n",
            "    Uninstalling urllib3-1.25.11:\n",
            "      Successfully uninstalled urllib3-1.25.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.6.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.0 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.7 wsproto-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfpevECLBhU5"
      },
      "source": [
        "#from bs4 import BeautifulSoup as bs\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv9E_QF1SuFm"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "australia = ['Victoria','Northern Territory','Australian Capital Territory','New South Wales','Western Australia','Tasmania','South Australia','Queensland']\n",
        "\n",
        "brazil = ['Goiás','Rondônia','Espírito Santo','Rio Grande do Norte','Tocantins','Santa Catarina','Roraima','Rio Grande do Sul','Paraíba','Piauí','Mato Grosso','Paraná',\n",
        "          'Amapá','Rio de Janeiro','São Paulo','Minas Gerais','Pará','Bahia','Distrito Federal','Pernambuco','Mato Grosso do Sul','Maranhão','Amazonas','Alagoas','Ceará',\n",
        "          'Acre','Sergipe']\n",
        "\n",
        "Canada = ['Yukon','Manitoba','Northwest Territories','Saskatchewan','New Brunswick','Quebec','Alberta','British Columbia','Ontario','Nova Scotia','Prince Edward Island',\n",
        "          'Newfoundland and Labrador','Nunavut']\n",
        "\n",
        "france = ['Martinique','French Guiana','Provence-Alpes-Côte dAzur',\n",
        "          'Corse','Pays de la Loire','Guadeloupe','Centre-Val de Loire','La Réunion','Occitanie','Hauts-de-France','Grand Est','Nouvelle-Aquitaine','Bourgogne-Franche-Comté',\n",
        "          'Île-de-France','Auvergne-Rhône-Alpes','Normandie','Bretagne','Mayotte','Orne','Ardèche','Jura','Bouches-du-Rhône','Eure-et-Loir','Creuse','Hérault','Hautes-Pyrénées'\n",
        "          ,'Aisne','Yonne','Haute-Corse','Maine-et-Loire','Vaucluse','Territoire de Belfort','Rhône','Haut-Rhin','Lozère','Ardennes','Corrèze','Val-dOise','Tarn-et-Garonne',\n",
        "          'Pyrénées-Orientales','Allier','Drôme','Saône-et-Loire','Moselle','Val-de-Marne','Loire','Meuse','Aveyron','Hautes-Alpes','Alpes-Maritimes','Savoie','Lot-et-Garonne',\n",
        "          'Oise','Doubs','Bas-Rhin','Gers','Nord','Var','Lot','Yvelines','Ille-et-Vilaine','Sarthe','Paris','Seine-Saint-Denis','Aube','Cantal','Haute-Loire','Loiret',\n",
        "          'Charente','Pyrénées-Atlantiques','Morbihan','Vienne','Nièvre','Gard','Loire-Atlantique','Haute-Vienne','Hauts-de-Seine','Haute-Savoie','Isère','Haute-Garonne','Essonne',\n",
        "          'Marne','Meurthe-et-Moselle','Haute-Marne','Seine-et-Marne','Ariège','Gironde','Mayenne','Corse-du-Sud','Vendée','Puy-de-Dôme','Cher','Loir-et-Cher','Indre-et-Loire',\n",
        "          'Tarn','Charente-Maritime','Seine-Maritime','Calvados','Landes','Eure','Indre','Alpes-de-Haute-Provence','Manche','Vosges','Aude','Pas-de-Calais','Côte-dOr','Finistère',\n",
        "          'Ain','Côtes-dArmor','Dordogne','Haute-Saône','Deux-Sèvres','Somme']\n",
        "\n",
        "Germany = ['Saxony','Bavaria','Thuringia','Saxony-Anhalt','Brandenburg','Baden-Württemberg','Berlin','Saarland','Rhineland-Palatinate','Hamburg','Hesse',\n",
        "           'Mecklenburg-Western Pomerania','North Rhine-Westphalia','Bremen','Lower Saxony','Schleswig-Holstein']\n",
        "India = ['Mizoram','Kerala','Ladakh','Puducherry','Manipur','Goa','Himachal Pradesh','Jammu and Kashmir','Lakshadweep','Tamil Nadu','Sikkim','Meghalaya','West Bengal',\n",
        "         'Maharashtra','Assam','Odisha','Nagaland','Karnataka','Telangana','Andhra Pradesh','Arunachal Pradesh','Chandigarh','Delhi','Andaman and Nicobar Islands','Tripura',\n",
        "         'Uttarakhand','Chhattisgarh','Punjab','Gujarat','Haryana','Jharkhand','Rajasthan','Madhya Pradesh','Bihar','Uttar Pradesh','Dadra and Nagar Haveli and Daman and Diu']\n",
        "\n",
        "Italy = ['Friuli Venezia Giulia','Trentino-Alto Adige','Valle dAosta',\n",
        "         'Veneto','Marche','Emilia-Romagna','Lazio','Liguria','Campania','Abruzzo','Lombardy','Piedmont','Tuscany','Sicily','Umbria','Calabria','Molise','Sardinia',\n",
        "         'Puglia','Basilicata','Trieste','Gorizia','Bolzano','Forlì-Cesena','Padua','Rimini','Aosta','Ravenna','Treviso','Venice','Vicenza','Pordenone','Udine',\n",
        "         'Imperia','Ascoli Piceno','Belluno','Fermo','Rovigo','La Spezia','Bologna','Rome','Grosseto','Verona','Teramo','Trento','Verbano-Cusio-Ossola','Varese']\n",
        "\n",
        "\n",
        "\n",
        "Japan = ['Okayama','Hokkaido','Osaka','Kanagawa','Kyoto','Gunma','Fukuoka','Saitama','Tokyo','Hyogo','Tochigi','Shiga','Aichi','Chiba','Hiroshima','Nara','Okinawa','Ibaraki','Saga',\n",
        "         'Gifu','Shizuoka','Yamagata','Niigata','Yamaguchi','Wakayama','Ishikawa','Mie','Tokushima','Kumamoto','Kagawa','Iwate','Ehime','Fukushima','Miyagi','Akita','Aomori',\n",
        "         'Fukui','Kagoshima','Kochi','Miyazaki','Nagano','Nagasaki','Oita','Shimane','Tottori','Toyama','Yamanashi']\n",
        "\n",
        "Mexico = ['Baja California','Sonora','Baja California Sur','Coahuila','Mexico City','Guanajuato','Chihuahua','Aguascalientes','Querétaro','San Luis Potosí','Yucatán','Nuevo León',\n",
        "          'Tabasco','Durango','Morelos','Tamaulipas','Oaxaca','Sinaloa','Zacatecas','Nayarit','State of Mexico','Colima','Jalisco','Hidalgo','Quintana Roo','Veracruz','Campeche',\n",
        "          'Michoacán','Guerrero','Puebla','Tlaxcala','Chiapas']\n",
        "Spain = ['Navarra','País Vasco','Aragón','Cataluña','Baleares','La Rioja','Murcia','Castilla y León','C. Valenciana','Canarias','Madrid','Cantabria','Galicia','Castilla La Mancha',\n",
        "         'Andalucía','Asturias','Extremadura','Ceuta','Melilla']\n",
        "England = ['Rutland','Devon','Telford and Wrekin','Torbay','Leicestershire','West Sussex','Wiltshire','Shropshire','Bournemouth, Christchurch and Poole','South Gloucestershire',\n",
        "           'Central Bedfordshire','Surrey','Plymouth','North Yorkshire','Dorset','West Berkshire','Bedford','Windsor and Maidenhead','Gloucestershire','Isle of Wight',\n",
        "           'North Lincolnshire','Buckinghamshire','Hampshire','Bath and North East Somerset','Hertfordshire','East Sussex','Bracknell Forest','Southampton','North East Lincolnshire',\n",
        "           'Staffordshire','Darlington','Milton Keynes','Portsmouth','Wokingham','Cheshire West and Chester','Oxfordshire','East Riding of Yorkshire','Thurrock','Essex','Herefordshire',\n",
        "           'York','Kent','Northamptonshire','Stoke-on-Trent','Peterborough','Brighton and Hove','Lincolnshire','Warwickshire','Kingston upon Hull','Somerset','Medway','Worcestershire',\n",
        "           'Derbyshire','Nottinghamshire','Reading','Luton','Halton','Cornwall and Isles of Scilly','Stockton-on-Tees','Cambridgeshire','North Somerset','Cheshire East','Southend-on-Sea',\n",
        "           'Bristol','Lancashire','Leicester','Slough','County Durham','Northumberland','Warrington','Swindon','Suffolk','South Yorkshire','Tyne and Wear','Greater Manchester','Merseyside',\n",
        "           'Hartlepool','Derby','Redcar and Cleveland','Blackpool','Nottingham','Norfolk','Blackburn with Darwen','Cumbria','West Midlands','Greater London','West Yorkshire','Middlesbrough']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjVt_viYT-jE",
        "outputId": "98fa5102-bb42-4ca0-bb7b-1c27da2405b4"
      },
      "source": [
        "%%writefile app.py\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from requests_html import HTMLSession \n",
        "import pickle\n",
        "import streamlit as st\n",
        "from tensorflow import keras\n",
        "# univariate bidirectional lstm example\n",
        "from numpy import array \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta, date\n",
        "from geopy.geocoders import Nominatim\n",
        "# Library for opening url and creating\n",
        "# requests\n",
        "import urllib.request\n",
        " \n",
        "# pretty-print python data structures\n",
        "from pprint import pprint\n",
        " \n",
        "# for parsing all the tables present\n",
        "# on the website\n",
        "from html_table_parser.parser import HTMLTableParser\n",
        "import builtins\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "country_lst = ['Aruba','Afghanistan','Angola','Albania','Andorra','United Arab Emirates','Argentina','Armenia','American Samoa',\n",
        " 'Antigua and Barbuda','Australia','Austria','Azerbaijan','Burundi','Belgium','Benin','Burkina Faso','Bangladesh','Bulgaria','Bahrain','Bahamas',\n",
        " 'Bosnia and Herzegovina','Belarus','Belize','Bermuda','Bolivia','Brazil','Barbados','Brunei','Bhutan','Botswana','Canada','Switzerland','Chile',\n",
        " 'China','Ivory Coast','Cameroon','Republic of the Congo','Cook Islands','Colombia','Comoros','Cape Verde','Costa Rica','Cuba','Curaçao','Cayman Islands',\n",
        " 'Cyprus','Czech Republic','Germany','Djibouti','Dominica','Denmark','Dominican Republic','Algeria','Ecuador','Egypt','Spain','Estonia','Ethiopia','Finland',\n",
        " 'Fiji','Falkland Islands','France','Faroe Islands','Gabon','United Kingdom','Guernsey','Ghana','Gibraltar','Guinea','Gambia','Guinea-Bissau',\n",
        " 'Equatorial Guinea','Greece','Grenada','Guatemala','French Guiana','Guam','Guyana','Hong Kong','Honduras','Croatia','Haiti','Hungary','Indonesia',\n",
        " 'Isle of Man','India','Ireland','Iran','Iraq','Iceland','Israel','Italy','Jamaica','Jersey','Jordan','Japan','Kazakhstan','Kenya','Kyrgyzstan','Cambodia',\n",
        " 'Kiribati','Saint Kitts and Nevis','South Korea','Kuwait','Laos','Lebanon','Liberia','Libya','Saint Lucia','Liechtenstein','Sri Lanka','Lesotho','Lithuania',\n",
        " 'Luxembourg','Latvia','Macau','Morocco','Monaco','Moldova','Madagascar','Maldives','Mexico','Marshall Islands','Mali','Malta','Myanmar','Montenegro',\n",
        " 'Mongolia','Northern Mariana Islands','Mozambique','Mauritania','Montserrat','Martinique','Mauritius','Malawi','Malaysia','Mayotte','New Caledonia','Niger',\n",
        " 'Nigeria','Nicaragua','Niue','Netherlands','Norway','Nepal','New Zealand','Oman','Pakistan','Panama','Peru','Philippines','Palau','Papua New Guinea','Poland',\n",
        " 'Puerto Rico','North Korea','Portugal','Paraguay','French Polynesia','Qatar','Réunion','Kosovo','Romania','Russia','Rwanda','Saudi Arabia','Sudan','Senegal',\n",
        " 'Singapore','Saint Helena','Solomon Islands','Sierra Leone','El Salvador','San Marino','Serbia','South Sudan','São Tomé and Príncipe','Suriname','Slovakia',\n",
        " 'Slovenia','Sweden','Swaziland','Sint Maarten','Syria','Turks and Caicos Islands','Chad','Togo','Thailand','Tajikistan','Tokelau','Turkmenistan','East Timor',\n",
        " 'Tonga','Trinidad and Tobago','Tunisia','Turkey','Tuvalu','Taiwan','Tanzania','Uganda','Ukraine','Uruguay','United States of America','Uzbekistan',\n",
        " 'Vatican City','Saint Vincent and the Grenadines','Venezuela','British Virgin Islands','United States Virgin Islands','Vietnam','Vanuatu',\n",
        " 'Wallis and Futuna','Samoa','Yemen','South Africa','Zambia','Zimbabwe']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "isos=['ABW','AFG','AGO','AIA','ALB','AND','ARE','ARG','ARM','ASM','ATG','AUS','AUT','AZE','BDI','BEL','BEN','BFA','BGD','BGR','BHR','BHS','BIH',\n",
        "'BLR','BLZ','BMU','BOL','BRA','BRB','BRN','BTN','BWA','CAN','CHE','CHL','CHN','CIV','CMR','COG','COK','COL','COM','CPV','CRI','CUB','CUW',\n",
        "'CYM','CYP','CZE','DEU','DJI','DMA','DNK','DOM','DZA','ECU','EGY','ESP','EST','ETH','FIN','FJI','FLK','FRA','FRO','GAB','GBR','GGY','GHA',\n",
        "'GIB','GIN','GMB','GNB','GNQ','GRC','GRD','GTM','GUF','GUM','GUY','HKG','HND','HRV','HTI','HUN','IDN','IMN','IND','IRL','IRN','IRQ','ISL',\n",
        "'ISR','ITA','JAM','JEY','JOR','JPN','KAZ','KEN','KGZ','KHM','KIR','KNA','KOR','KWT','LAO','LBN','LBR','LBY','LCA','LIE','LKA','LSO','LTU',\n",
        "'LUX','LVA','MAC','MAR','MCO','MDA','MDG','MDV','MEX','MHL','MLI','MLT','MMR','MNE','MNG','MNP','MOZ','MRT','MSR','MTQ','MUS','MWI','MYS',\n",
        "'MYT','NCL','NER','NGA','NIC','NIU','NLD','NOR','NPL','NZL','OMN','PAK','PAN','PER','PHL','PLW','PNG','POL','PRI','PRK','PRT','PRY','PYF',\n",
        "'QAT','REU','RKS','ROU','RUS','RWA','SAU','SDN','SEN','SGP','SHN','SLB','SLE','SLV','SMR','SRB','SSD','STP','SUR','SVK','SVN','SWE','SWZ',\n",
        "'SXM','SYR','TCA','TCD','TGO','THA','TJK','TKL','TKM','TLS','TON','TTO','TUN','TUR','TUV','TWN','TZA','UGA','UKR','URY','USA','UZB','VAT',\n",
        "'VCT','VEN','VGB','VIR','VNM','VUT','WLF','WSM','YEM','ZAF','ZMB','ZWE']\n",
        "\n",
        "# loading the trained model\n",
        "model = keras.models.load_model('https://github.com/neerja198/Deployment/blob/main/finalmodel.h5')\n",
        "#regression = pickle.load(model)\n",
        "\n",
        "@st.cache()\n",
        "\n",
        "\n",
        "# session = requests.Session()\n",
        "def getWeather(city_name,country_name):\n",
        "  \n",
        "  \n",
        "  session = HTMLSession()\n",
        "  headers = {'Accept-Language': 'en-US,en;q=0.8'}\n",
        "  base_link = \"https://www.google.com/search?q=weather/\"+ city_name + \"+\" +country_name\n",
        "  \n",
        "  response = session.get(base_link)\n",
        "  soup = bs(response.content, 'html.parser')\n",
        "\n",
        "  #return base_link\n",
        "  \n",
        "  temp =  soup.find(\"span\", attrs={\"id\": \"wob_tm\"}).text\n",
        " \n",
        "  region = soup.find(\"div\", attrs={\"id\": \"wob_loc\"}).text\n",
        "          \n",
        "          # store all results on this dictionary\n",
        "  next_days = []\n",
        "  days = soup.find(\"div\", attrs={\"id\": \"wob_dp\"})\n",
        "  for day in days.findAll(\"div\", attrs={\"class\": \"wob_df\"}):\n",
        "    day_name = day.findAll(\"div\")[0].attrs['aria-label']\n",
        "              # get weather status for that day\n",
        "    weather_img = day.find(\"img\").attrs[\"src\"]\n",
        "    weather_desc = day.find(\"img\").attrs[\"alt\"]\n",
        "    maxtemp = day.findAll(\"div\", {\"class\": \"gNCp2e\"})\n",
        "    max_temp = maxtemp[0].text\n",
        "    mintemp = day.findAll(\"div\", {\"class\": \"QrNVmd ZXCv8e\"})\n",
        "    min_temp = mintemp[0].text\n",
        "    next_days.append({\"name\": day_name, \"weather_desc\": weather_desc, \"weather_img\":weather_img , \"max_temp\": max_temp[2:], \"min_temp\": min_temp[2:]})\n",
        "  return next_days,temp,region,base_link\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prediction(city_name,country_name,iso_code,label_alpha_2,select_location):\n",
        "  session = HTMLSession()\n",
        "  headers = {'Accept-Language': 'en-US,en;q=0.8'}\n",
        "\n",
        "  location = select_location.replace(\" \", \"-\")\n",
        "  country = location.lower()\n",
        " \n",
        "  usa_prov = country_name.lower()\n",
        "  #scraping the button show all click data\n",
        "  #return select_location\n",
        "  wd = webdriver.Chrome('chromedriver',options=options)\n",
        "  if select_location == 'USA':\n",
        "      wd.get(\"https://www.nytimes.com/interactive/2021/us/\"+usa_prov+\"-covid-cases.html\")\n",
        "      #baseurl = \"https://www.nytimes.com/interactive/2021/us/\"+usa_prov+\"-covid-cases.html\"\n",
        "      #return baseurl\n",
        "  else:\n",
        "      wd.get(\"https://www.nytimes.com/interactive/2021/world/\"+country+\"-covid-cases.html\")\n",
        "      #baseurl =\"https://www.nytimes.com/interactive/2021/world/\"+country+\"-covid-cases.html\"\n",
        "      #print(baseurl)\n",
        "   \n",
        "\n",
        "\n",
        "  #element = wd.find_element(By.XPATH,\"//button[contains(@class,'showall')]\")\n",
        "  #element = wd.find_element(By.XPATH,\"//*[@id='world-france-covid-cases']/div/div/main/div[3]/div[4]/section[2]/div[2]/table/tbody/tr\")\n",
        "  #wd.execute_script(\"arguments[0].click();\", element)\n",
        "\n",
        "  elements=wd.find_elements_by_xpath(\"//button[contains(@class,'showall')]\")\n",
        "  for element in elements:\n",
        "    wd.execute_script(\"arguments[0].click();\", element);\n",
        "    #return element.text\n",
        "  # Get 'HTML'\n",
        "\n",
        "  html_data = wd.page_source\n",
        "  #return html_data\n",
        "  soup = bs(html_data, 'html.parser')\n",
        "  table = soup.findAll('table', {\"class\": \"g-table super-table withchildren\"})\n",
        "  #return table\n",
        "  if len(table) == 2:\n",
        "\n",
        "      df = pd.read_html(str(table[0]))[0]\n",
        "      df = df[df.columns[:2]]\n",
        "      df = df.rename(columns={ df.columns[0]: \"location\", df.columns[1]: \"new_cases\" })\n",
        "      #if country=='italy':\n",
        "       # df.drop(columns=['Per 100,000',\t'14-day change'], inplace=True)\n",
        "       # df.rename(columns={'Unnamed: 0': 'location', 'Cases Daily Avg.':'new_cases'}, inplace=True)\n",
        "      #else:\n",
        "       # df.drop(columns=['Per 100,000',\t'14-day change',\t'Deaths Daily Avg.',\t'Per 100,000.1'], inplace=True)\n",
        "       # df.rename(columns={'Unnamed: 0': 'location', 'New Hospitalizations Daily Avg.':'new_cases'}, inplace=True)\n",
        "      \n",
        "      df2 = pd.read_html(str(table[1]))[0]\n",
        "\n",
        "\n",
        "      df2 = df2[df2.columns[:2]]\n",
        "      df2 = df2.rename(columns={ df2.columns[0]: \"location\", df2.columns[1]: \"new_cases\" })\n",
        "      #if country=='italy':\n",
        "       # df2.drop(columns=['Per 100,000',\t'14-day change'], inplace=True)\n",
        "       # df2.rename(columns={'Unnamed: 0': 'location', 'Cases Daily Avg.':'new_cases'}, inplace=True)\n",
        "      #else:\n",
        "       # df2.drop(columns=['Per 100,000',\t'14-day change',\t'Deaths Daily Avg.',\t'Per 100,000.1'], inplace=True)\n",
        "       # df2.rename(columns={'Unnamed: 0': 'location', 'New Hospitalizations Daily Avg.':'new_cases'}, inplace=True)\n",
        "\n",
        "      df = df.append(df2)\n",
        "    \n",
        "\n",
        "  else:\n",
        "\n",
        "      df = pd.read_html(str(table[0]))[0]\n",
        "      df = df[df.columns[:2]]\n",
        "      df = df.rename(columns={ df.columns[0]: \"location\", df.columns[1]: \"new_cases\" })\n",
        "      #df.drop(columns=['Per 100,000',\t'14-day change',\t'Deaths Daily Avg.',\t'Per 100,000.1'], inplace=True)\n",
        "      #df.rename(columns={'Unnamed: 0': 'location', 'Cases Daily Avg.':'new_cases'}, inplace=True)\n",
        "\n",
        "  #return df  \n",
        "  if select_location == 'USA':  \n",
        "      df['location'] = df['location'].replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
        "      #city =  df[df['location'] == city_name]  \n",
        "      new_cases = df[df['location'] == city_name]['new_cases'].values[0]\n",
        "     \n",
        "  else:\n",
        "      #city =  df[df['location'] == city_name]   \n",
        "      new_cases = df[df['location'] == city_name]['new_cases'].values[0]\n",
        "     \n",
        "\n",
        "      \n",
        "  if str(new_cases).startswith('<'): \n",
        "      new_cases = int(new_cases[1:])\n",
        "  else :\n",
        "      int(new_cases)\n",
        "\n",
        "  #return new_cases \n",
        "  #call the weather function\n",
        "  \n",
        "  weatherData = getWeather(city_name,country_name)\n",
        " # return weatherData\n",
        "  #prepare temp to pass into model\n",
        "  temp = int(weatherData[1])  \n",
        "           \n",
        "  lengthnewcases = len(str(new_cases))\n",
        "\n",
        "        #preparation of model data\n",
        "  final_prediction = []  \n",
        "  n_steps = 1\n",
        "  n_features = 3\n",
        "  #max_temp = 45\n",
        "  #min_temp = 29\n",
        "  x_input = array([int(iso_code), int(new_cases), float(temp)])\n",
        "  x_input = x_input.reshape((1, n_steps, n_features))   \n",
        "      # Making predictions \n",
        "  prediction = model.predict(x_input)\n",
        "              #formatting the prediction values\n",
        "  #return prediction\n",
        "            \n",
        "  for i in prediction: \n",
        "      \n",
        "      for j in i:\n",
        "        out = f'{j:.10f}'\n",
        "        out = out.split('.')[1]\n",
        "                  # Strip the zeros from the left side of your split decimal\n",
        "        out = out.lstrip('0')\n",
        "        final_no = out[:lengthnewcases]           \n",
        "        final_prediction.append(final_no)\n",
        "\n",
        "        #print(f\"{int(final_no):,}\")\n",
        "\n",
        "            \n",
        "  return final_prediction,select_location,weatherData,new_cases,label_alpha_2,city_name\n",
        "       \n",
        "\n",
        "import base64\n",
        "   \n",
        "\n",
        "# this is the main function in which we define our webpage  \n",
        "def main():  \n",
        "    # front end elements of the web page \n",
        "    st.set_page_config(layout=\"wide\")\n",
        "    main_bg = \"https://raw.githubusercontent.com/neerja198/Deployment/main/giphy.gif\"\n",
        "    main_bg_ext = \"gif\"\n",
        "    padding_left = \"50px\"\n",
        "    #background-color: #1c294b\n",
        "    #background: url(data:image/{main_bg_ext};base64,{base64.b64encode(open(main_bg, \"rb\").read()).decode()});\n",
        "    #background-size: cover;\n",
        "    #background-position: center;\n",
        "    #backdrop-filter: blur(5px);\n",
        "    st.sidebar.markdown(\n",
        "      f\"\"\"\n",
        "      <style>\n",
        "      \n",
        "      .reportview-container .main {{\n",
        "\n",
        "          background: url(data:image/{main_bg_ext};base64,{base64.b64encode(open(main_bg, \"rb\").read()).decode()});\n",
        "          \n",
        "          background-size: 150px 100px;\n",
        "          background-repeat: repeat-y;\n",
        "          background-color: #031444;\n",
        "          \n",
        "\n",
        "          }}\n",
        "\n",
        "     \n",
        "      </style>\n",
        "      \"\"\",\n",
        "      unsafe_allow_html=True\n",
        "    )\n",
        "     \n",
        "  \n",
        "   \n",
        "    st.sidebar.image(\"https://raw.githubusercontent.com/neerja198/Deployment/main/tlogo.png\")\n",
        "    Date_today = date.today()  \n",
        "       \n",
        "    html_date = str(\"<p style='color: white; text-align:center; font-size:20px'>\") + str(Date_today.strftime(\"%d %b, %y\"))+ str(\"</p>\")\n",
        "    st.sidebar.markdown(html_date, unsafe_allow_html=True)\n",
        "   \n",
        "    st.markdown(\n",
        "      f\"\"\"\n",
        "      <style>\n",
        "      [data-testid=\"stSidebar\"][aria-expanded=\"true\"] > div:first-child {{\n",
        "         \n",
        "          background-color:#031444;\n",
        "          width:230px;\n",
        "          \n",
        "      }}\n",
        "      </style>\n",
        "      \"\"\",\n",
        "      unsafe_allow_html=True,\n",
        "      )\n",
        "    \n",
        "    \n",
        "    new_title = '<p style=\"font-style: oblique; text-align:center;color:#ffe168; font-size:25px\">5  Days Forecasting of Covid 19 New Cases Based on Weather</p>'\n",
        "    st.markdown(new_title,unsafe_allow_html=True)\n",
        "   \n",
        "  \n",
        "\n",
        "    #adding sidebar\n",
        "\n",
        "    #-- Set by location\n",
        "    sel_location = '<p style=\"font-style: oblique; text-align:left;color:white; font-size:16px\">Select Location</p>'\n",
        "    st.sidebar.markdown(sel_location,unsafe_allow_html=True)\n",
        "    st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    [data-baseweb=\"select\"] {\n",
        "        margin-top: -50px;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        "    )\n",
        "    select_location = st.sidebar.selectbox('Select Location',\n",
        "                                    ['Australia', 'Brazil','Canada','France','Germany','Italy','India','Japan','Mexico','Spain','United Kingdom','USA'])\n",
        "    \n",
        "                             \n",
        "\n",
        "    if select_location == 'USA':\n",
        "            usa_dataset = pd.read_csv('https://raw.githubusercontent.com/neerja198/Deployment/main/USA_referance.csv') \n",
        "           \n",
        "            sel_prov = '<p style=\"font-style: oblique; text-align:left;color:white; font-size:16px\">Select Province</p>'\n",
        "            st.sidebar.markdown(sel_prov,unsafe_allow_html=True)\n",
        "            us_state = st.sidebar.selectbox(\"Select Province\", sorted(usa_dataset.State.unique()), index=0)\n",
        "            sel_city = '<p style=\"font-style: oblique; text-align:left;color:white; font-size:16px\">Select City</p>'\n",
        "            st.sidebar.markdown(sel_city,unsafe_allow_html=True)\n",
        "            city_name = st.sidebar.selectbox(\"Select City\", sorted(usa_dataset.loc[usa_dataset.State == us_state].City.unique()))\n",
        "           \n",
        "            city_name = city_name.replace(\"'\", \"\")\n",
        "            \n",
        "            # 0: city   1: country   2: ISO   3: Label\n",
        "            us_reference =usa_dataset.loc[usa_dataset.City== city_name].values[0]\n",
        "          \n",
        "            country_name = us_state\n",
        "            iso_code = us_reference[4]\n",
        "            alpha_2 = us_reference[2] \n",
        "            label_alpha_2 = alpha_2[:2].lower()\n",
        "\n",
        "\n",
        "            result =\"\"\n",
        "\n",
        "    else: \n",
        "          reference_file = pd.read_csv('https://raw.githubusercontent.com/neerja198/Deployment/main/location_list.csv')\n",
        "          \n",
        "\n",
        "          #country = st.sidebar.selectbox(\"Select Country\", sorted(reference_file.Country.unique()), index=0)\n",
        "          country = select_location\n",
        "          sel_prov_city = '<p style=\"font-style: oblique; text-align:left;color:white; font-size:16px\">Select Province/City</p>'\n",
        "          st.sidebar.markdown(sel_prov_city,unsafe_allow_html=True)\n",
        "          city_name = st.sidebar.selectbox(\"Select Province/City\", sorted(reference_file.loc[reference_file.Country == country].City.unique()))    \n",
        "          city_name = city_name.replace(\"'\", \"\")\n",
        "          \n",
        "          # 0: city   1: country   2: ISO   3: Label\n",
        "          city_reference =reference_file.loc[reference_file.City== city_name].values[0]\n",
        "        \n",
        "          country_name = city_reference[1]\n",
        "          country_name = country_name.lower()\n",
        "          iso_code = city_reference[3]\n",
        "          alpha_2 = city_reference[2] \n",
        "          label_alpha_2 = alpha_2[:2].lower()\n",
        "\n",
        "\n",
        "          result =\"\"\n",
        "\n",
        "    # when 'Predict' is clicked, make the prediction and store it \n",
        "    if st.sidebar.button(\"Predict\"): \n",
        "        result = prediction(city_name,country_name,iso_code,label_alpha_2,select_location)\n",
        "        #st.success(result)\n",
        "        if len(result) == 6:\n",
        "            forecast_weather = result[2]\n",
        "            forecast_weather = forecast_weather[:2]\n",
        "           \n",
        "            max_temp = [d['max_temp'] for d in forecast_weather[0] if 'max_temp' in d]\n",
        "            min_temp = [d['min_temp'] for d in forecast_weather[0] if 'min_temp' in d]\n",
        "            weather_desc = [d['weather_desc'] for d in forecast_weather[0] if 'weather_desc' in d]\n",
        "            weather_img = [d['weather_img'] for d in forecast_weather[0] if 'weather_img' in d]\n",
        "\n",
        "           \n",
        "            col1,col2= st.columns(2)\n",
        "            Date_today = date.today()  \n",
        "            \n",
        "            html_location = str(\"<p style='text-align: left; color: white; font-size:20px'>\") + str( result[5] + \" , \"  +result[1])+ str(\"</p>\")\n",
        "            col1.markdown(html_location, unsafe_allow_html=True)\n",
        "            if result[4] == 'fl':\n",
        "                st.image(\"https://flagcdn.com/256x192/\"+ \"us-\" +result[4]+\".png\" , width=40)\n",
        "            else:\n",
        "                st.image(\"https://flagcdn.com/256x192/\"+result[4]+\".png\" , width=40)\n",
        "            html_newcases = str(\"<p style='text-align: right; color: white; font-size:20px'>\") + str( \"New Cases: \" + f\"{int(result[3]):,}\")+ str(\"</p>\")\n",
        "            col2.markdown(html_newcases, unsafe_allow_html=True)\n",
        "            \n",
        "            \n",
        "            #st.metric(\"New Cases\", f\"{int(result[3]):,}\") \n",
        "            #st.metric(\"Location\", result[1].title())\n",
        "        \n",
        "\n",
        "            forecast_newcases = result[0]\n",
        "            #print(forecast_newcases)\n",
        "            forecast_newcases.insert(0,result[3])\n",
        "            \n",
        "            \n",
        "            association_msg = []\n",
        "            for i in builtins.range(0,5):\n",
        "               if int(forecast_newcases[i]) < int(forecast_newcases[i+1]):\n",
        "                  association_msg.append(\"Today the weather is \" + weather_desc[i] + \" and temp is between \" + min_temp[i] + \"/ \" + max_temp[i] + \" , So the number of new cases of covid-19 cases may increase.\")\n",
        "\n",
        "               elif int(forecast_newcases[i]) > int(forecast_newcases[i+1]):\n",
        "                  association_msg.append(\"Today the weather is \" + weather_desc[i] + \" and temp is between \" + min_temp[i] + \"/ \" + max_temp[i] + \" , So the number of new cases of covid-19 cases may decrease.\")\n",
        "\n",
        "               else:\n",
        "                  association_msg.append(\"Today the weather is \" + weather_desc[i] + \" and temp is between \" + min_temp[i] + \"/ \" + max_temp[i] + \", So the number of new cases of covid-19 cases may not be affected.\")\n",
        "           \n",
        "           \n",
        "            format_nc = []\n",
        "            for x in forecast_newcases:\n",
        "              x = int(x)\n",
        "              if x < 4:\n",
        "                minimum_cases = 0\n",
        "                maximum_cases = x\n",
        "                new_cases = str(minimum_cases)+\" - \"+str(maximum_cases)\n",
        "                format_nc.append(new_cases)\n",
        "              else:\n",
        "                range = x/4\n",
        "                range = int(range)\n",
        "                minimum_cases = x-range\n",
        "                maximum_cases = x + range\n",
        "                cases_range = \"between \" + str(minimum_cases)+\" and \"+str(maximum_cases)\n",
        "                new_cases = str(minimum_cases)+\" - \"+str(maximum_cases)\n",
        "                format_nc.append(new_cases)\n",
        "            #st.success(format_nc)\n",
        "            \n",
        "            #date list\n",
        "            date_lst = []\n",
        "           \n",
        "\n",
        "            for i in builtins.range(0,5):\n",
        "                Date_req = Date_today+ timedelta(days=i+1)\n",
        "                date_lst.append(Date_req.strftime(\"%d %b %y\"))\n",
        "\n",
        "\n",
        "            \n",
        "                \n",
        "            \n",
        "            #index_value = ['Date','New Cases','Icon','Desc','Max_Temp','Min_Temp']\n",
        "            #tble for date\n",
        "\n",
        "            html_code1 = str(\"<h4 style='text-align: center; color: white;'>\") + str(date_lst[0])+ str(\"</h4>\")\n",
        "            html_code2 = str(\"<h4 style='text-align: center; color: white;'>\") + str(date_lst[1])+ str(\"</h4>\")\n",
        "            html_code3 = str(\"<h4 style='text-align: center; color: white;'>\") + str(date_lst[2])+ str(\"</h4>\")\n",
        "            html_code4 = str(\"<h4 style='text-align: center; color: white;'>\") + str(date_lst[3])+ str(\"</h4>\")\n",
        "            html_code5 = str(\"<h4 style='text-align: center; color: white;'>\") + str(date_lst[4])+ str(\"</h4>\")\n",
        "\n",
        "           \n",
        "            col9,col10,col11,col12,col13 = st.columns(5)\n",
        "            col9.markdown(html_code1, unsafe_allow_html=True)\n",
        "            col10.markdown(html_code2, unsafe_allow_html=True)\n",
        "            col11.markdown(html_code3, unsafe_allow_html=True)\n",
        "            col12.markdown(html_code4, unsafe_allow_html=True)\n",
        "            col13.markdown(html_code5, unsafe_allow_html=True)\n",
        "\n",
        "            #table for new cases\n",
        "            html_code6 = str(\"<h3 style='text-align: center; color: #ffe168;'>\") +str(format_nc[1] )+ str(\"</h3>\")\n",
        "            html_code7 = str(\"<h3 style='text-align: center; color: #ffe168;'>\") + str(format_nc[2])+ str(\"</h3>\")\n",
        "            html_code8 = str(\"<h3 style='text-align: center; color: #ffe168;'>\") + str(format_nc[3])+ str(\"</h3>\")\n",
        "            html_code9 = str(\"<h3 style='text-align: center; color: #ffe168;'>\") + str(format_nc[4])+ str(\"</h3>\")\n",
        "            html_code10 = str(\"<h3 style='text-align: center; color: #ffe168;'>\") + str(format_nc[5])+ str(\"</h3>\")\n",
        "\n",
        "           \n",
        "            col14,col15,col16,col17,col18 = st.columns(5)\n",
        "            col14.markdown(html_code6,unsafe_allow_html=True)\n",
        "            \n",
        "            col15.markdown(html_code7,unsafe_allow_html=True)\n",
        "            col16.markdown(html_code8,unsafe_allow_html=True)\n",
        "            col17.markdown(html_code9,unsafe_allow_html=True)\n",
        "            col18.markdown(html_code10,unsafe_allow_html=True)\n",
        "\n",
        "            html_ass1 = str(\"<p style='text-align: center; color: white;'>\") +str(association_msg[0])+ str(\"</p>\")\n",
        "            html_ass2= str(\"<p style='text-align: center; color: white;'>\") +str(association_msg[1] )+ str(\"</p>\")\n",
        "            html_ass3= str(\"<p style='text-align: center; color: white;'>\") +str(association_msg[2] )+ str(\"</p>\")\n",
        "            html_ass4= str(\"<p style='text-align: center; color: white;'>\") +str(association_msg[3] )+ str(\"</p>\")\n",
        "            html_ass5= str(\"<p style='text-align: center; color: white;'>\") +str(association_msg[4] )+ str(\"</p>\")\n",
        "\n",
        "            col_ass1,col_ass2,col_ass3,col_ass4,col_ass5 = st.columns(5)\n",
        "            col_ass1.markdown(html_ass1,unsafe_allow_html=True)\n",
        "            col_ass2.markdown(html_ass2,unsafe_allow_html=True)\n",
        "            col_ass3.markdown(html_ass3,unsafe_allow_html=True)\n",
        "            col_ass4.markdown(html_ass4,unsafe_allow_html=True)\n",
        "            col_ass5.markdown(html_ass5,unsafe_allow_html=True)\n",
        "\n",
        "            #table for Desc\n",
        "            html_code11 = str(\"<p style='text-align: center; font-style: oblique; color: white; font-size:20px'>\") + str(weather_desc[0])+ str(\"</p>\")\n",
        "            html_code12 = str(\"<p style='text-align: center; font-style: oblique; color: white; font-size:20px'>\") + str(weather_desc[1])+ str(\"</p>\")\n",
        "            html_code13 = str(\"<p style='text-align: center; font-style: oblique; color: white; font-size:20px'>\") + str(weather_desc[2])+ str(\"</p>\")\n",
        "            html_code14 = str(\"<p style='text-align: center; font-style: oblique; color: white; font-size:20px'>\") + str(weather_desc[3])+ str(\"</p>\")\n",
        "            html_code15 = str(\"<p style='text-align: center; font-style: oblique; color: white; font-size:20px'>\") + str(weather_desc[4])+ str(\"</p>\")\n",
        "\n",
        "           \n",
        "            col19,col20,col21,col22,col23 = st.columns(5)\n",
        "            col19.markdown(html_code11, unsafe_allow_html=True)\n",
        "            col20.markdown(html_code12, unsafe_allow_html=True)\n",
        "            col21.markdown(html_code13, unsafe_allow_html=True)\n",
        "            col22.markdown(html_code14, unsafe_allow_html=True)\n",
        "            col23.markdown(html_code15, unsafe_allow_html=True)\n",
        "            \n",
        "            \n",
        "\n",
        "            #table for icon\n",
        "\n",
        "            col24,col25,col26,col27,col28,col24_1,col24_2,col24_3,col24_4,col24_5 = st.columns([1,2,1,2,1,2,1,2,1,2])\n",
        "           \n",
        "           \n",
        "        \n",
        "            with col25:\n",
        "                st.image(\"https:\"+weather_img[0])\n",
        "            with col27:\n",
        "                st.image(\"https:\"+weather_img[1])\n",
        "            with col24_1:\n",
        "                st.image(\"https:\"+weather_img[2])\n",
        "            with col24_3:\n",
        "                st.image(\"https:\"+weather_img[3])\n",
        "            with col24_5:\n",
        "                st.image(\"https:\"+weather_img[4])\n",
        "            \n",
        "            \n",
        "            \n",
        "            #col26.image(\"https:\"+weather_img[1])\n",
        "            #col27.image(\"https:\"+weather_img[2])\n",
        "            #col28.image(\"https:\"+weather_img[3])\n",
        "            #col34.image(\"https:\"+weather_img[4])\n",
        "\n",
        "            #table for mintemp/maxtemp\n",
        "            html_code21 = str(\"<p style='text-align: center; color: white; font-size:20px'>\") + str(min_temp[0] + \"C\"+ \"/\"+max_temp[0]+ \"C\")+ str(\"</p>\")\n",
        "            html_code22 = str(\"<p style='text-align: center; color: white; font-size:20px'>\") + str(min_temp[1] + \"C\" + \" / \"+max_temp[1]+ \"C\")+ str(\"</p>\")\n",
        "            html_code23 = str(\"<p style='text-align: center; color: white; font-size:20px'>\") + str(min_temp[2] + \"C\" + \" / \"+max_temp[2]+ \"C\")+ str(\"</p>\")\n",
        "            html_code24 = str(\"<p style='text-align: center; color: white; font-size:20px'>\") + str(min_temp[3] + \"C\" + \" / \"+max_temp[3]+ \"C\")+ str(\"</p>\")\n",
        "            html_code25 = str(\"<p style='text-align: center; color: white; font-size:20px'>\") + str(min_temp[4] + \"C\" + \" / \"+max_temp[4]+ \"C\")+ str(\"</p>\")\n",
        "\n",
        "           \n",
        "            col29,col30,col31,col32,col33 = st.columns(5)\n",
        "            col29.markdown(html_code21,unsafe_allow_html=True)\n",
        "            col30.markdown(html_code22,unsafe_allow_html=True)\n",
        "            col31.markdown(html_code23,unsafe_allow_html=True)\n",
        "            col32.markdown(html_code24,unsafe_allow_html=True)\n",
        "            col33.markdown(html_code25,unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.success(result)  \n",
        "\n",
        "            \n",
        "            \n",
        "            #st.success(\"Date : \" + \"  \"+\"  \"+ Date_req.strftime(\"%d %b, %Y\") + \"  \" + \" , \" + \"New Cases :\" + \" \" + f\"{int(forecast_newcases[i]):,}\" + \" , \"\n",
        "             #+  \"Max_Temp: \" + \" \" + max_temp[i] + \"°C\"+\",\" +  \"Min_Temp: \" + \" \" + min_temp[i] + \"°C\"+\",\" + \"Desc: \" + \" \" + weather[i] )\n",
        "     \n",
        "     \n",
        "if __name__=='__main__': \n",
        "    main()\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2_CyYdrUzeM"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1apskpmHcWJW"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdtS_VqMccdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32fad801-dfea-4bdf-8de8-cf4347f651a2"
      },
      "source": [
        "from pyngrok import ngrok\n",
        " \n",
        "public_url = ngrok.connect('8501')\n",
        "public_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://d175-35-237-232-134.ngrok.io\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}